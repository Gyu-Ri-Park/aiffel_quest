{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6aad0d",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|:--------|:--------|\n",
    "|1. 한국어 전처리 과정이 적절하였는가?|형태소 분석기 선택과 불용어 제거가 체계적으로 진행됨|\n",
    "|2. 텍스트 데이터 수집이 분량과 다양성 측면에서 적절했는가?|일자와 분량에서 텍스트 데이터 다양성 향상을 위한 노력이 확인됨|\n",
    "|3. 분류모델의 test accuracy가 기준 이상 높게 나왔는가?|F-1 score 기준 83% 이상의 정확도가 확인됨|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc3eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1148962",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 수를 지정해 주세요.\n",
      "페이지수: 2\n",
      "카테고리 코드를 입력해 주세요. (예: 100, 101, 102)\n",
      "100:정치, 101:경제, 102:사회, 103:생활/문화, 104:세계, 105:IT/과학\n",
      "카테고리 코드: 102, 105\n",
      "크롤링을 시작할 날짜를 00000000형식으로 지정하세요.\n",
      "크롤링 시작 날짜:20231231\n",
      "크롤링을 마칠 날짜를 00000000형식으로 지정하세요.\n",
      "크롤링 종료 날짜:20240101\n"
     ]
    }
   ],
   "source": [
    "### 사용자 입력 처리\n",
    "print(\"페이지 수를 지정해 주세요.\")\n",
    "input_page = input(\"페이지수: \")\n",
    "page_num = int(input_page)\n",
    "\n",
    "print(\"카테고리 코드를 입력해 주세요. (예: 100, 101, 102)\")\n",
    "print(\"100:정치, 101:경제, 102:사회, 103:생활/문화, 104:세계, 105:IT/과학\")\n",
    "input_code = input(\"카테고리 코드: \")\n",
    "code_list = [int(code.strip()) for code in input_code.split(',')]\n",
    "\n",
    "print(\"크롤링을 시작할 날짜를 00000000형식으로 지정하세요.\")\n",
    "input_sdate = input(\"크롤링 시작 날짜:\")\n",
    "\n",
    "print(\"크롤링을 마칠 날짜를 00000000형식으로 지정하세요.\")\n",
    "input_edate = input(\"크롤링 종료 날짜:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507cbb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31부터 2024-01-01까지의 데이터를 크롤링합니다.\n"
     ]
    }
   ],
   "source": [
    "### 날짜 데이터 분리\n",
    "def parse_date(date_str):\n",
    "    year = date_str[:4]\n",
    "    month = date_str[4:6]\n",
    "    day = date_str[6:]\n",
    "    formatted_date = f\"{year}-{month}-{day}\"\n",
    "    return datetime.strptime(formatted_date, '%Y-%m-%d')\n",
    "\n",
    "start_date = parse_date(input_sdate)\n",
    "end_date = parse_date(input_edate)\n",
    "\n",
    "\n",
    "### 시작 일자부터 종료 일자까지의 날짜를 리스트로 생성\n",
    "date_list = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_list.append(current_date.strftime('%Y-%m-%d'))\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "print(f\"{start_date.strftime('%Y-%m-%d')}부터 {end_date.strftime('%Y-%m-%d')}까지의 데이터를 크롤링합니다.\")\n",
    "\n",
    "int_date_list = [int(date.replace('-', '')) for date in date_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9e89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\n",
    "def make_urllist(page_num, code, date): \n",
    "  urllist= []\n",
    "  for i in range(1, page_num + 1):\n",
    "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}\n",
    "    news = requests.get(url, headers=headers)\n",
    "\n",
    "    # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "    soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "    # CASE 1\n",
    "    news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "    # CASE 2\n",
    "    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "    for line in news_list:\n",
    "        urllist.append(line.a.get('href'))\n",
    "  return urllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7549bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 URL을 저장할 리스트\n",
    "all_urllist = []\n",
    "\n",
    "# 각 카테고리에 대한 데이터프레임을 저장할 변수 초기화\n",
    "df = pd.DataFrame()\n",
    "def make_data(urllist, code):\n",
    "    text_list = []\n",
    "    for url in urllist:\n",
    "        article = Article(url, language='ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text_list.append(article.title)\n",
    "    df = pd.DataFrame({'news': text_list})\n",
    "    df['code'] = idx2word[str(code)]\n",
    "    return df\n",
    "\n",
    "# 코드로부터 바로 어떤 카테고리인지 확인하기 쉽도록 딕셔너리 생성\n",
    "idx2word = {'100': '정치', '101': '경제', '102': '사회', '103': '생활/문화', '104': '세계', '105': 'IT/과학'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for code in code_list:\n",
    "    code_urllist = []  # 각 카테고리에 대한 URL 리스트\n",
    "\n",
    "    for date in int_date_list:\n",
    "        urllist = make_urllist(page_num, code, date)\n",
    "        code_urllist.extend(urllist)  # 각 날짜별 URL을 code_urllist에 추가\n",
    "\n",
    "    all_urllist.extend(code_urllist)  # 각 카테고리별 URL 리스트를 all_urllist에 추가\n",
    "    df_temp = make_data(code_urllist, code)\n",
    "    print(str(code) + '번 코드에 대한 데이터를 만들었습니다.')\n",
    "\n",
    "    df = pd.concat([df, df_temp])  # 각 카테고리별 데이터프레임을 df에 추가\n",
    "\n",
    "print(\"총\", len(all_urllist), \"개의 기사를 수집했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거합니다.\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161bc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 확인\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복된 샘플들을 제거\n",
    "df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "print('뉴스 기사의 개수: ',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "# Hannanum 형태소 분석기 초기화\n",
    "tokenizer = Hannanum()\n",
    "\n",
    "# 불용어 리스트 정의\n",
    "stopwords = ['에', '는', '을', '를', '이', '가', '도', '다', '의', '한', '과', '와', '으로', '하다', ]\n",
    "\n",
    "# 데이터 전처리 및 토큰화 함수\n",
    "def preprocessing(data):\n",
    "    text_data = []\n",
    "\n",
    "    for sentence in data:\n",
    "        temp_data = tokenizer.morphs(sentence)  # 토큰화\n",
    "        temp_data = [word for word in temp_data if not word in stopwords]  # 불용어 제거\n",
    "        text_data.append(temp_data)\n",
    "\n",
    "    text_data = list(map(' '.join, text_data))\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# 샘플 출력\n",
    "text_data = preprocessing(df['news'])\n",
    "print(text_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85473b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d009c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46da724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b80d6",
   "metadata": {},
   "source": [
    "마지막에 커널과의 연결이 끊겨서 output 데이터를 출력하지 못했는데 IT/과학쪽은 f-score가 0.75였습니다\n",
    "생각보다 구현이 간단한 듯 복잡하네요\n",
    "더 많은 공부가 필요할 것 같습니다..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aa5190",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
