{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d63b5e",
   "metadata": {},
   "source": [
    "|평가문항\t|상세기준|\n",
    "|:-|:-|\n",
    "|1. 모델과 데이터를 정상적으로 불러오고, 작동하는 것을 확인하였다.\t|klue/bert-base를 NSMC 데이터셋으로 fine-tuning 하여, 모델이 정상적으로 작동하는 것을 확인하였다.|\n",
    "|2. Preprocessing을 개선하고, fine-tuning을 통해 모델의 성능을 개선시켰다.\t|Validation accuracy를 90% 이상으로 개선하였다.|\n",
    "|3. 모델 학습에 Bucketing을 성공적으로 적용하고, 그 결과를 비교분석하였다.\t|Bucketing task을 수행하여 fine-tuning 시 연산 속도와 모델 성능 간의 trade-off 관계가 발생하는지 여부를 확인하고, 분석한 결과를 제시하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7663c",
   "metadata": {},
   "source": [
    "STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성  \n",
    "---\n",
    "데이터셋은 깃허브에서 다운받거나, Huggingface datasets에서 가져올 수 있습니다. 앞에서 배운 방법들을 활용해봅시다!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6a5e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "#라이브러리 버전 확인\n",
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09e703f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/807 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset nsmc/default (download: 18.62 MiB, generated: 20.90 MiB, post-processed: Unknown size, total: 39.52 MiB) to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nsmc downloaded and prepared to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# nsmc 데이터셋 다운로드\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "nsmc_dataset = load_dataset('nsmc')\n",
    "print(nsmc_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e9b42",
   "metadata": {},
   "source": [
    "NSMC dataset\n",
    "- Dataset dictionary 안에 train dataset, test dataset으로 구성\n",
    "- 각 dataset은 'id', 'document', 'label'로 구성되어 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d170dcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train datasets의 각 컬럼에 해당하는 몇 가지 요소들을 확인하며 제대로 짝지어져있는지 확인\n",
    "train = nsmc_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2105c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86889bd2",
   "metadata": {},
   "source": [
    "긍정적인 평가는 1, 부정적인 평가는 0으로 라벨링되어 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f9dad",
   "metadata": {},
   "source": [
    "STEP 2. klue/bert-base model 및 tokenizer 불러오기  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36dc220",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d18794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'document': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(nsmc_dataset['train'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0ce77",
   "metadata": {},
   "source": [
    "STEP 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859d5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 모델이 처리할 수 있는 형식으로 전처리합니다.\n",
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'], \n",
    "        truncation=True, # 긴 문장을 자르기 위해\n",
    "        padding='max_length', # 모든 문장을 동일한 길이로 패딩\n",
    "        return_token_type_ids=False, # NSMC는 단일 문장이므로 token_type_ids는 필요 없음\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d236585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 한번에 토크나이징할 수 있는 Map\n",
    "tokenized_datasets = nsmc_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5374363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split\n",
    "hf_train_dataset = tokenized_datasets['train']\n",
    "hf_test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db688716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(hf_train_dataset))\n",
    "print(len(hf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57fbd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split 10%만 샘플링\n",
    "hf_train_10 = tokenized_datasets['train'].shuffle(seed=42).select(range(15000))\n",
    "hf_test_10 = tokenized_datasets['test'].shuffle(seed=42).select(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b32b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e19e9f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    # 예측값 중 가장 높은 확률을 가진 클래스를 선택\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # 실제 레이블과 예측값을 비교하여 정확도를 계산\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f1ed09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:22:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.313898</td>\n",
       "      <td>0.877200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.450585</td>\n",
       "      <td>0.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.588901</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5625, training_loss=0.2501371673583984, metrics={'train_runtime': 4967.0742, 'train_samples_per_second': 9.06, 'train_steps_per_second': 1.132, 'total_flos': 1.18399974912e+16, 'train_loss': 0.2501371673583984, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # 앞서 정의한 모델 인스턴스\n",
    "    args=training_arguments,             # TrainingArguments 인스턴스\n",
    "    train_dataset=hf_train_10,      # 훈련 데이터셋\n",
    "    eval_dataset=hf_test_10,        # 검증 데이터셋, 별도로 정의한 검증 데이터셋이 있으면 그것을 사용하세요.\n",
    "    compute_metrics=compute_metrics,     # 성능 평가를 위한 메트릭 계산 함수\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf06d06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 29:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5949050188064575,\n",
       " 'eval_accuracy': 0.87902,\n",
       " 'eval_runtime': 1766.999,\n",
       " 'eval_samples_per_second': 28.297,\n",
       " 'eval_steps_per_second': 3.537,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05223fe",
   "metadata": {},
   "source": [
    "정확도 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9df57a",
   "metadata": {},
   "source": [
    "STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기  \n",
    "---\n",
    "데이터 전처리, TrainingArguments 등을 조정하여 모델의 정확도를 90% 이상으로 끌어올려봅시다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "997ae913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-7de5a5cea8f6e64d.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-a462fab3c6f8b9b5.arrow\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# NSMC 데이터셋 로드\n",
    "dataset = load_dataset('nsmc')\n",
    "\n",
    "# 정규 표현식을 사용하여 한글과 띄어쓰기를 제외한 모든 문자 제거 함수 정의\n",
    "def remove_non_korean(text):\n",
    "    return re.sub(r'[^가-힣\\s]', '', text)\n",
    "\n",
    "# 데이터셋의 'document' 필드에 전처리 함수 적용\n",
    "def preprocess_dataset(example):\n",
    "    example['document'] = remove_non_korean(example['document'])\n",
    "    return example\n",
    "\n",
    "# 훈련 및 테스트 데이터셋에 전처리 함수 적용\n",
    "preprocessed_dataset = dataset.map(preprocess_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da087321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙 진짜 짜증나네요 목소리\n",
      "흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나\n"
     ]
    }
   ],
   "source": [
    "# 전처리된 데이터 중 일부 출력해 확인\n",
    "print(preprocessed_dataset['train'][0]['document'])\n",
    "print(preprocessed_dataset['train'][1]['document'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce71b3",
   "metadata": {},
   "source": [
    "[링크](https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc)를 참고하여 모델 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4d758ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/8dfd196dc647eeecc3a02e5fdcba2371f264501c2935144aabc462888716a112.40a5c8401f1996599a7dae4e5c50067a9c93277a116a5dd33536d29bd041b1a0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": null,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a2b3f9492eb9f3e88e19bb94f604643e746f36240217357c18126c14bff0b1e.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/2fa2fe26d08c7be570a4ed0b59e642282b9d3fe9d91cc2e4928525135918c253.8cc8735eb415b2e32573ba7745bcd5ae2cc0da3a44f9cfd6713a443520322950\n",
      "loading file https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/33d72f2e2414a00763933b4d4fc49fc27804450550c94340cccd98875348f505.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/8e21af421d1eabfe95bd2133700be9db26bc67e68617c862393f6850e155a44f.5f5f6f23b457ba934e33bb426b26ac4cdcd75edd25fc1e12068d48aa8b68806e\n",
      "loading configuration file https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/8dfd196dc647eeecc3a02e5fdcba2371f264501c2935144aabc462888716a112.40a5c8401f1996599a7dae4e5c50067a9c93277a116a5dd33536d29bd041b1a0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": null,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/8dfd196dc647eeecc3a02e5fdcba2371f264501c2935144aabc462888716a112.40a5c8401f1996599a7dae4e5c50067a9c93277a116a5dd33536d29bd041b1a0\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": null,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/17b6c0c881f4d94f6e7cb635339e2c078680b5d83580f3f2067ed3c9f3f784a7.29f7ad8f3828b89ac8fe9e121c10090724d215cfa6c3bb8cf2f270091a7f58bf\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at sangrimlee/bert-base-multilingual-cased-nsmc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sangrimlee/bert-base-multilingual-cased-nsmc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"sangrimlee/bert-base-multilingual-cased-nsmc\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c638ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 모델이 처리할 수 있는 형식으로 전처리합니다.\n",
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'], \n",
    "        truncation=True, # 긴 문장을 자르기 위해\n",
    "        padding='max_length', # 모든 문장을 동일한 길이로 패딩\n",
    "        return_token_type_ids=False, # NSMC는 단일 문장이므로 token_type_ids는 필요 없음\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c75b3216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-cf4c0b31a4f78dde.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 한번에 토크나이징할 수 있는 Map\n",
    "tokenized_datasets = preprocessed_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d13286a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split\n",
    "hf_train_dataset = tokenized_datasets['train']\n",
    "hf_test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "864db278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(hf_train_dataset))\n",
    "print(len(hf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "973b33c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-49429d2e47f86562.arrow\n"
     ]
    }
   ],
   "source": [
    "# train & test split 10%만 샘플링\n",
    "hf_train_10 = tokenized_datasets['train'].shuffle(seed=42).select(range(15000))\n",
    "hf_test_10 = tokenized_datasets['test'].shuffle(seed=42).select(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2fce420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "984c0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    # 예측값 중 가장 높은 확률을 가진 클래스를 선택\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # 실제 레이블과 예측값을 비교하여 정확도를 계산\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7f60b68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:28:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>0.383188</td>\n",
       "      <td>0.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>0.599115</td>\n",
       "      <td>0.837800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.747193</td>\n",
       "      <td>0.841800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5625, training_loss=0.3052540863037109, metrics={'train_runtime': 5293.906, 'train_samples_per_second': 8.5, 'train_steps_per_second': 1.063, 'total_flos': 1.18399974912e+16, 'train_loss': 0.3052540863037109, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # 앞서 정의한 모델 인스턴스\n",
    "    args=training_arguments,             # TrainingArguments 인스턴스\n",
    "    train_dataset=hf_train_10,      # 훈련 데이터셋\n",
    "    eval_dataset=hf_test_10,        # 검증 데이터셋, 별도로 정의한 검증 데이터셋이 있으면 그것을 사용하세요.\n",
    "    compute_metrics=compute_metrics,     # 성능 평가를 위한 메트릭 계산 함수\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4395d118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 29:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7426859140396118,\n",
       " 'eval_accuracy': 0.8439,\n",
       " 'eval_runtime': 1763.2928,\n",
       " 'eval_samples_per_second': 28.356,\n",
       " 'eval_steps_per_second': 3.545,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e8c83",
   "metadata": {},
   "source": [
    "정확도 88 -> 84 하락. 전처리는 동일하게 하고, 모델을 다시 KLUE로 바꿔서 테스트."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b2655c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-7de5a5cea8f6e64d.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-a462fab3c6f8b9b5.arrow\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# NSMC 데이터셋 로드\n",
    "dataset = load_dataset('nsmc')\n",
    "\n",
    "# 정규 표현식을 사용하여 한글과 띄어쓰기를 제외한 모든 문자 제거 함수 정의\n",
    "def remove_non_korean(text):\n",
    "    return re.sub(r'[^가-힣\\s]', '', text)\n",
    "\n",
    "# 데이터셋의 'document' 필드에 전처리 함수 적용\n",
    "def preprocess_dataset(example):\n",
    "    example['document'] = remove_non_korean(example['document'])\n",
    "    return example\n",
    "\n",
    "# 훈련 및 테스트 데이터셋에 전처리 함수 적용\n",
    "preprocessed_dataset = dataset.map(preprocess_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79dd5c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙 진짜 짜증나네요 목소리\n",
      "흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나\n"
     ]
    }
   ],
   "source": [
    "# 전처리된 데이터 중 일부 출력해 확인\n",
    "print(preprocessed_dataset['train'][0]['document'])\n",
    "print(preprocessed_dataset['train'][1]['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a68c32c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3664186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 모델이 처리할 수 있는 형식으로 전처리합니다.\n",
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'], \n",
    "        truncation=True, # 긴 문장을 자르기 위해\n",
    "        padding='max_length', # 모든 문장을 동일한 길이로 패딩\n",
    "        return_token_type_ids=False, # NSMC는 단일 문장이므로 token_type_ids는 필요 없음\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38ec40dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 한번에 토크나이징할 수 있는 Map\n",
    "tokenized_datasets = preprocessed_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30001902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split\n",
    "hf_train_dataset = tokenized_datasets['train']\n",
    "hf_test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09f34381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(hf_train_dataset))\n",
    "print(len(hf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64803d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split 10%만 샘플링\n",
    "hf_train_10 = tokenized_datasets['train'].shuffle(seed=42).select(range(15000))\n",
    "hf_test_10 = tokenized_datasets['test'].shuffle(seed=42).select(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f9aa51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ed311eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    # 예측값 중 가장 높은 확률을 가진 클래스를 선택\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # 실제 레이블과 예측값을 비교하여 정확도를 계산\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b042636",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:23:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.356400</td>\n",
       "      <td>0.326528</td>\n",
       "      <td>0.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.447977</td>\n",
       "      <td>0.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.585166</td>\n",
       "      <td>0.873200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5625, training_loss=0.2701903527153863, metrics={'train_runtime': 5029.2468, 'train_samples_per_second': 8.948, 'train_steps_per_second': 1.118, 'total_flos': 1.18399974912e+16, 'train_loss': 0.2701903527153863, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # 앞서 정의한 모델 인스턴스\n",
    "    args=training_arguments,             # TrainingArguments 인스턴스\n",
    "    train_dataset=hf_train_10,      # 훈련 데이터셋\n",
    "    eval_dataset=hf_test_10,        # 검증 데이터셋, 별도로 정의한 검증 데이터셋이 있으면 그것을 사용하세요.\n",
    "    compute_metrics=compute_metrics,     # 성능 평가를 위한 메트릭 계산 함수\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3c179af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 29:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5886006951332092,\n",
       " 'eval_accuracy': 0.86882,\n",
       " 'eval_runtime': 1765.81,\n",
       " 'eval_samples_per_second': 28.316,\n",
       " 'eval_steps_per_second': 3.539,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a4190",
   "metadata": {},
   "source": [
    "88 -> 84 -> 87%.  \n",
    "klue-bert 대신 다른 모델 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ea3cf47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/356388226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a213507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-7de5a5cea8f6e64d.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-a462fab3c6f8b9b5.arrow\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# NSMC 데이터셋 로드\n",
    "dataset = load_dataset('nsmc')\n",
    "\n",
    "# 정규 표현식을 사용하여 한글과 띄어쓰기를 제외한 모든 문자 제거 함수 정의\n",
    "def remove_non_korean(text):\n",
    "    return re.sub(r'[^가-힣\\s]', '', text)\n",
    "\n",
    "# 데이터셋의 'document' 필드에 전처리 함수 적용\n",
    "def preprocess_dataset(example):\n",
    "    example['document'] = remove_non_korean(example['document'])\n",
    "    return example\n",
    "\n",
    "# 훈련 및 테스트 데이터셋에 전처리 함수 적용\n",
    "preprocessed_dataset = dataset.map(preprocess_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "889cd049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙 진짜 짜증나네요 목소리\n",
      "흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나\n"
     ]
    }
   ],
   "source": [
    "# 전처리된 데이터 중 일부 출력해 확인\n",
    "print(preprocessed_dataset['train'][0]['document'])\n",
    "print(preprocessed_dataset['train'][1]['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "36008d46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmpdgcrumb3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json in cache at /aiffel/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
      "https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmp5koqhl4l\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt in cache at /aiffel/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmp6iojone1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/734k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json in cache at /aiffel/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmphmgujtp6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json in cache at /aiffel/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
      "https://huggingface.co/klue/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmpn73x2cwm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/klue/roberta-base/resolve/main/config.json in cache at /aiffel/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
      "loading configuration file https://huggingface.co/klue/roberta-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmpgl8piyq8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/422M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin in cache at /aiffel/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
      "loading weights file https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9f6711b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 모델이 처리할 수 있는 형식으로 전처리합니다.\n",
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'], \n",
    "        truncation=True, # 긴 문장을 자르기 위해\n",
    "        padding='max_length', # 모든 문장을 동일한 길이로 패딩\n",
    "        return_token_type_ids=False, # NSMC는 단일 문장이므로 token_type_ids는 필요 없음\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2f9d53b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 한번에 토크나이징할 수 있는 Map\n",
    "tokenized_datasets = preprocessed_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "482ec4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split\n",
    "hf_train_dataset = tokenized_datasets['train']\n",
    "hf_test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6fe94c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(hf_train_dataset))\n",
    "print(len(hf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "89d4e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split 10%만 샘플링\n",
    "hf_train_10 = tokenized_datasets['train'].shuffle(seed=42).select(range(15000))\n",
    "hf_test_10 = tokenized_datasets['test'].shuffle(seed=42).select(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dfac939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c53c168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    # 예측값 중 가장 높은 확률을 가진 클래스를 선택\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # 실제 레이블과 예측값을 비교하여 정확도를 계산\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6cd03ff6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:23:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.384400</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.441062</td>\n",
       "      <td>0.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.180300</td>\n",
       "      <td>0.540630</td>\n",
       "      <td>0.873200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5625, training_loss=0.3058576107449002, metrics={'train_runtime': 5036.7885, 'train_samples_per_second': 8.934, 'train_steps_per_second': 1.117, 'total_flos': 1.18399974912e+16, 'train_loss': 0.3058576107449002, 'epoch': 3.0})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # 앞서 정의한 모델 인스턴스\n",
    "    args=training_arguments,             # TrainingArguments 인스턴스\n",
    "    train_dataset=hf_train_10,      # 훈련 데이터셋\n",
    "    eval_dataset=hf_test_10,        # 검증 데이터셋, 별도로 정의한 검증 데이터셋이 있으면 그것을 사용하세요.\n",
    "    compute_metrics=compute_metrics,     # 성능 평가를 위한 메트릭 계산 함수\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "98e61870",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 25:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5392884612083435,\n",
       " 'eval_accuracy': 0.8712,\n",
       " 'eval_runtime': 1549.4886,\n",
       " 'eval_samples_per_second': 32.269,\n",
       " 'eval_steps_per_second': 4.034,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2fc07",
   "metadata": {},
   "source": [
    "동일한 전처리 작업을 진행했을 때에는 klue/roberta-base의 성능이 가장 좋았다. (다 비슷하긴 하지만...)  \n",
    "좀 더 전처리해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30b0ff80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "174a0800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e0581d34204e78a7499d0d9763e887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_dataset at 0x7f288636f4c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c477e25ab6841d8a5b45276ad4da817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcaec3e62d64da98f9db3edd2696d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙 진짜 짜증나다 목소리\n",
      "흠 포스터 보고 초딩 영화 줄 오버 연기 조차 가볍다 않다\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# KoNLPy의 Okt 객체 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# NSMC 데이터셋 로드\n",
    "dataset = load_dataset('nsmc')\n",
    "\n",
    "# 불용어 리스트 정의\n",
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 한글과 띄어쓰기를 제외한 모든 문자 제거\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    # 숫자 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # 어근 추출 및 불용어 제거\n",
    "    tokenized = okt.morphs(text, stem=True)  # 어근 추출\n",
    "    tokenized = [word for word in tokenized if not word in stopwords]  # 불용어 제거\n",
    "    return ' '.join(tokenized)\n",
    "\n",
    "# 데이터셋 전처리 적용 함수\n",
    "def preprocess_dataset(example):\n",
    "    example['document'] = preprocess_text(example['document'])\n",
    "    return example\n",
    "\n",
    "# 훈련 및 테스트 데이터셋에 전처리 함수 적용\n",
    "preprocessed_dataset = dataset.map(preprocess_dataset)\n",
    "\n",
    "# 전처리된 데이터 중 일부 출력해 확인\n",
    "print(preprocessed_dataset['train'][0]['document'])\n",
    "print(preprocessed_dataset['train'][1]['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "39aa328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
      "loading configuration file https://huggingface.co/klue/roberta-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4abda70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 모델이 처리할 수 있는 형식으로 전처리합니다.\n",
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'], \n",
    "        truncation=True, # 긴 문장을 자르기 위해\n",
    "        padding='max_length', # 모든 문장을 동일한 길이로 패딩\n",
    "        return_token_type_ids=False, # NSMC는 단일 문장이므로 token_type_ids는 필요 없음\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4ce48a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d289862f4914a84bca326a7153c4ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c362c7d73b2e4d6aac703e97558c6232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 한번에 토크나이징할 수 있는 Map\n",
    "tokenized_datasets = preprocessed_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "02b02bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split\n",
    "hf_train_dataset = tokenized_datasets['train']\n",
    "hf_test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "743707da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(hf_train_dataset))\n",
    "print(len(hf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cff18622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split 10%만 샘플링\n",
    "hf_train_10 = tokenized_datasets['train'].shuffle(seed=42).select(range(15000))\n",
    "hf_test_10 = tokenized_datasets['test'].shuffle(seed=42).select(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ecba4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a85aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    # 예측값 중 가장 높은 확률을 가진 클래스를 선택\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # 실제 레이블과 예측값을 비교하여 정확도를 계산\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3e0de02b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:24:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.416900</td>\n",
       "      <td>0.386791</td>\n",
       "      <td>0.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.321600</td>\n",
       "      <td>0.465901</td>\n",
       "      <td>0.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>0.581474</td>\n",
       "      <td>0.853800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5625, training_loss=0.33857879028320315, metrics={'train_runtime': 5042.8082, 'train_samples_per_second': 8.924, 'train_steps_per_second': 1.115, 'total_flos': 1.18399974912e+16, 'train_loss': 0.33857879028320315, 'epoch': 3.0})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # 앞서 정의한 모델 인스턴스\n",
    "    args=training_arguments,             # TrainingArguments 인스턴스\n",
    "    train_dataset=hf_train_10,      # 훈련 데이터셋\n",
    "    eval_dataset=hf_test_10,        # 검증 데이터셋, 별도로 정의한 검증 데이터셋이 있으면 그것을 사용하세요.\n",
    "    compute_metrics=compute_metrics,     # 성능 평가를 위한 메트릭 계산 함수\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f60f155f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 25:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5909972190856934,\n",
       " 'eval_accuracy': 0.8512,\n",
       " 'eval_runtime': 1545.1997,\n",
       " 'eval_samples_per_second': 32.358,\n",
       " 'eval_steps_per_second': 4.045,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68760a9e",
   "metadata": {},
   "source": [
    "전처리를 할수록 성능이 떨어짐...모델은 바꾸고 전처리는 최소화, 샘플 수를 늘려서 마지막 테스트."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8a2b6550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5ba48bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "#라이브러리 버전 확인\n",
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "185f6757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f99813407a44bc98ab858e6dfe9c409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# nsmc 데이터셋 다운로드\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "nsmc_dataset = load_dataset('nsmc')\n",
    "print(nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c3c26a56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
      "loading configuration file https://huggingface.co/klue/roberta-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d7fe5aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'document': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(nsmc_dataset['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f263a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 모델이 처리할 수 있는 형식으로 전처리합니다.\n",
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'], \n",
    "        truncation=True, # 긴 문장을 자르기 위해\n",
    "        padding='max_length', # 모든 문장을 동일한 길이로 패딩\n",
    "        return_token_type_ids=False, # NSMC는 단일 문장이므로 token_type_ids는 필요 없음\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "04a0cb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-d12e978f9c0192a7.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116f6852cbb94ed8b4d895ad4ee0ef14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 한번에 토크나이징할 수 있는 Map\n",
    "tokenized_datasets = nsmc_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e011b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test split\n",
    "hf_train_dataset = tokenized_datasets['train']\n",
    "hf_test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ea6aed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(hf_train_dataset))\n",
    "print(len(hf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "904ecdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-6568446119b44cb0.arrow\n"
     ]
    }
   ],
   "source": [
    "# train & test split 20%만 샘플링\n",
    "hf_train_20 = tokenized_datasets['train'].shuffle(seed=42).select(range(30000))\n",
    "hf_test_20 = tokenized_datasets['test'].shuffle(seed=42).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6a01a5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 5,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "40a615cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    # 예측값 중 가장 높은 확률을 가진 클래스를 선택\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # 실제 레이블과 예측값을 비교하여 정확도를 계산\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "728b109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 30000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 4:34:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.323244</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.307300</td>\n",
       "      <td>0.419268</td>\n",
       "      <td>0.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.472737</td>\n",
       "      <td>0.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>0.581696</td>\n",
       "      <td>0.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>0.615328</td>\n",
       "      <td>0.890800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-6500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-6500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-7000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-7000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-7500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-7500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-7500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-8000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-8000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-8500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-8500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-9000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-9000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-9500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-9500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-10000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-10000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-10500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-10500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-11000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-11000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-11000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-11500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-11500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-12000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-12000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-12500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-12500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-13000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-13000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-13500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-13500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-14000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-14000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-14500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-14500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-15000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-15000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-15000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-15500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-15500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-16000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-16000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-16500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-16500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-17000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-17000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-17500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-17500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-18000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-18000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-18500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-18500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-18500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18750, training_loss=0.22518865641276042, metrics={'train_runtime': 16472.5261, 'train_samples_per_second': 9.106, 'train_steps_per_second': 1.138, 'total_flos': 3.9466658304e+16, 'train_loss': 0.22518865641276042, 'epoch': 5.0})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # 앞서 정의한 모델 인스턴스\n",
    "    args=training_arguments,             # TrainingArguments 인스턴스\n",
    "    train_dataset=hf_train_20,      # 훈련 데이터셋\n",
    "    eval_dataset=hf_test_20,        # 검증 데이터셋, 별도로 정의한 검증 데이터셋이 있으면 그것을 사용하세요.\n",
    "    compute_metrics=compute_metrics,     # 성능 평가를 위한 메트릭 계산 함수\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "92b40bbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 25:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6351636052131653,\n",
       " 'eval_accuracy': 0.88646,\n",
       " 'eval_runtime': 1546.5869,\n",
       " 'eval_samples_per_second': 32.329,\n",
       " 'eval_steps_per_second': 4.041,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb8405",
   "metadata": {},
   "source": [
    "90%까지는 달성하지 못했지만, 최대 약 89%까지 달성함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd726fd",
   "metadata": {},
   "source": [
    "STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교  \n",
    "---\n",
    "- 아래 링크를 바탕으로 bucketing과 dynamic padding이 무엇인지 알아보고, 이들을 적용하여 model을 학습시킵니다.  \n",
    "    - Data Collator\n",
    "    - Trainer.TrainingArguments 의 group_by_length   \n",
    "    \n",
    "STEP 4에 학습한 결과와 bucketing을 적용하여 학습시킨 결과를 비교해보고, 모델 성능 향상과 훈련 시간 두 가지 측면에서 각각 어떤 이점이 있는지 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "856029ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b53acefc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81019250e094460fb5aa4a8366c41927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
      "loading configuration file https://huggingface.co/klue/roberta-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62241f7028fd49f2b97095762e017b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cca2126592244bca8cfaf5274f19109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 30000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 4:24:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.474500</td>\n",
       "      <td>0.434299</td>\n",
       "      <td>0.841500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.755100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>0.536503</td>\n",
       "      <td>0.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.698553</td>\n",
       "      <td>0.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.443468</td>\n",
       "      <td>0.834500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-3750\n",
      "Configuration saved in ./results/checkpoint-3750/config.json\n",
      "Model weights saved in ./results/checkpoint-3750/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-7500\n",
      "Configuration saved in ./results/checkpoint-7500/config.json\n",
      "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-11250\n",
      "Configuration saved in ./results/checkpoint-11250/config.json\n",
      "Model weights saved in ./results/checkpoint-11250/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-15000\n",
      "Configuration saved in ./results/checkpoint-15000/config.json\n",
      "Model weights saved in ./results/checkpoint-15000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-18750\n",
      "Configuration saved in ./results/checkpoint-18750/config.json\n",
      "Model weights saved in ./results/checkpoint-18750/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-3750 (score: 0.8415).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18750, training_loss=0.4983467692057292, metrics={'train_runtime': 15880.7852, 'train_samples_per_second': 9.445, 'train_steps_per_second': 1.181, 'total_flos': 3.9466658304e+16, 'train_loss': 0.4983467692057292, 'epoch': 5.0})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "\n",
    "# NSMC 데이터셋 로드\n",
    "nsmc_dataset = load_dataset('nsmc')\n",
    "\n",
    "# 토크나이저 및 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels=2)\n",
    "\n",
    "# 데이터셋 전처리 함수\n",
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'], \n",
    "        truncation=True, # 긴 문장을 자르기 위해\n",
    "        padding='max_length', # 모든 문장을 동일한 길이로 패딩\n",
    "        return_token_type_ids=False, # NSMC는 단일 문장이므로 token_type_ids는 필요 없음\n",
    "    )\n",
    "\n",
    "# 데이터셋 한번에 토크나이징할 수 있는 Map\n",
    "tokenized_datasets = nsmc_dataset.map(transform, batched=True)\n",
    "\n",
    "# 데이터셋 분할, 20%만 샘플링\n",
    "train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(30000))\n",
    "test_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(10000))\n",
    "\n",
    "# 동적 패딩을 위한 데이터 콜레이터 정의\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',             # 결과가 저장될 경로\n",
    "    num_train_epochs=5,                 # 총 학습 에포크\n",
    "    per_device_train_batch_size=8,      # 디바이스 당 훈련 배치 크기\n",
    "    per_device_eval_batch_size=8,       # 디바이스 당 평가 배치 크기\n",
    "    warmup_steps=500,                   # 웜업 스텝 수\n",
    "    weight_decay=0.01,                  # 가중치 감소\n",
    "    logging_dir='./logs',               # 로깅이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",        # 에포크마다 평가\n",
    "    save_strategy=\"epoch\",              # 에포크마다 모델 저장\n",
    "    load_best_model_at_end=True,        # 학습 종료 시 가장 좋은 모델을 로드\n",
    "    metric_for_best_model=\"accuracy\",   # 최고 모델을 결정하기 위한 메트릭\n",
    "    group_by_length=True                # 유사한 길이의 샘플을 그룹화\n",
    ")\n",
    "\n",
    "# 정확도 계산 함수\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 트레이너 초기화\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "42f36f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 05:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4342985451221466,\n",
       " 'eval_accuracy': 0.8415,\n",
       " 'eval_runtime': 312.1579,\n",
       " 'eval_samples_per_second': 32.035,\n",
       " 'eval_steps_per_second': 4.004,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deec69f",
   "metadata": {},
   "source": [
    "bucketing과 dynamic padding을 적용하기 전의 결과는 다음과 같다.  \n",
    "```\n",
    "{'eval_loss': 0.6351636052131653,\n",
    " 'eval_accuracy': 0.88646,\n",
    " 'eval_runtime': 1546.5869,\n",
    " 'eval_samples_per_second': 32.329,\n",
    " 'eval_steps_per_second': 4.041,\n",
    " 'epoch': 5.0}\n",
    "```\n",
    "bucketing과 dynamic padding을 적용한 후의 정확도가 더 낮지만 loss가 더 작고 평가 시간이 1/5 정도로 단축되었다.  \n",
    "- 모델 선택 시 정확도, 손실, 평가 시간 등 여러 가지 지표를 고려해야 함\n",
    "- 특정 상황이나 목표에 따라 최적의 모델을 선택하는 전략이 달라질 수 있음\n",
    "    - 시간 효율성이 중요한 경우 두 번째 모델을, 정확도가 중요하다면 첫 번째 모델이 더 적합함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352be1a6",
   "metadata": {},
   "source": [
    "# 회고\n",
    "결국 90%까지 올리진 못했네요...아직도 트랜스포머를 잘 이해하지는 못했지만 뭔가 이런 식으로도 딥러닝을 할 수 있다는 건 조금은 이해한 것 같습니다.  \n",
    "90% 이상 올리신 분들은 무슨 모델 쓰셨나 궁금하네요ㅎㅎ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
